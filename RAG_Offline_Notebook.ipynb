{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f73901",
   "metadata": {},
   "source": [
    "## Notebook completo ‚Äî PDF ‚Üí Chunks ‚Üí Embeddings FAISS ‚Üí RAG Offline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f4bda",
   "metadata": {},
   "source": [
    "### üìå 1. Instalar Depend√™ncias\n",
    "(Cole numa c√©lula separada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\aulaml\\venv\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: faiss-cpu in c:\\aulaml\\venv\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\aulaml\\venv\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers in c:\\aulaml\\venv\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in c:\\aulaml\\venv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in c:\\aulaml\\venv\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\aulaml\\venv\\lib\\site-packages (from pdfplumber) (12.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\aulaml\\venv\\lib\\site-packages (from pdfplumber) (5.0.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\aulaml\\venv\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\aulaml\\venv\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\aulaml\\venv\\lib\\site-packages (from faiss-cpu) (2.3.5)\n",
      "Requirement already satisfied: packaging in c:\\aulaml\\venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: tqdm in c:\\aulaml\\venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\aulaml\\venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\aulaml\\venv\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\aulaml\\venv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\aulaml\\venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\aulaml\\venv\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\aulaml\\venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\aulaml\\venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\aulaml\\venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\aulaml\\venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\aulaml\\venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\aulaml\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\aulaml\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\aulaml\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\aulaml\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\aulaml\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\aulaml\\venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\aulaml\\venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\aulaml\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\aulaml\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\aulaml\\venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\aulaml\\venv\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\aulaml\\venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\aulaml\\venv\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\aulaml\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\aulaml\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instala√ß√£o das depend√™ncias necess√°rias\n",
    "%pip install pdfplumber faiss-cpu sentence-transformers transformers torch \n",
    "# %pip install --upgrade ipywidgets jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145408f7",
   "metadata": {},
   "source": [
    "### üìå 2. Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0a7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o das bibliotecas necess√°rias\n",
    "import pdfplumber\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b691f3d",
   "metadata": {},
   "source": [
    "### üìå 3. Fun√ß√£o para Ler PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c545670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fun√ß√µes carregadas.\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o para extrair texto de um PDF usando pdfplumber\n",
    "def ler_pdf(caminho_pdf):\n",
    "    texto = \"\"\n",
    "    with pdfplumber.open(caminho_pdf) as pdf:\n",
    "        for pagina in pdf.pages:\n",
    "            texto += pagina.extract_text() + \"\\n\"\n",
    "    return texto\n",
    "\n",
    "print(\"Fun√ß√µes carregadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56690f6e",
   "metadata": {},
   "source": [
    "### üìå 4. Dividir texto em chunks\n",
    "Chunk size ajust√°vel para n√£o estourar contexto do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2d3e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fun√ß√µes de chunk prontas.\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o para dividir o texto em peda√ßos (chunks)\n",
    "def criar_chunks(texto, tamanho=400, sobreposicao=50):\n",
    "    palavras = texto.split()\n",
    "    chunks = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(palavras):\n",
    "        chunk = palavras[i:i + tamanho]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        i += tamanho - sobreposicao\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"Fun√ß√µes de chunk prontas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03145b25",
   "metadata": {},
   "source": [
    "### üìå 5. Gerar Embeddings com modelo offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b65d280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64e7f56154a4a1cbf96ac0fae31f913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AULAML\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Erik\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6166a53075e043ef892b7c6342f915bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa42e4572cc641439ceb57ca71ded257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc70248344444e9bbdc123f6844744a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b28e1f8cb2420188faf572c276e6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942c2976ba8a48158d6c54fd0219753c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47757880e66949ff8b4aef88c06d2c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150506a31b9e49338ef42bf0301e6a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153f4bd360e54e6caab4d439919f0420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c218ad172124b5f9975e6d5b3307706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac80e53409145d980acd989e98166b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carrega modelo de embeddings local (n√£o precisa internet)\n",
    "modelo_emb = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def gerar_embeddings(lista_textos):\n",
    "    return modelo_emb.encode(lista_textos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c73c2d",
   "metadata": {},
   "source": [
    "### üìå 6. Criar e salvar o √≠ndice FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00df559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria √≠ndice FAISS baseado em similaridade\n",
    "def criar_faiss(embeddings, caminho_index=\"faiss_index.bin\"):\n",
    "    dim = embeddings.shape[1]  \n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, caminho_index)\n",
    "    print(\"Index salvo em:\", caminho_index)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ddc751",
   "metadata": {},
   "source": [
    "### üìå 7. Pipeline completo: PDF ‚Üí Chunks ‚Üí Embeddings ‚Üí FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7d77b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 chunks criados e salvos.\n",
      "Index salvo em: faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "# Caminho do PDF\n",
    "caminho_pdf = \"Little-Red-Riding-Hood.pdf\"  # arquivo na MESMA pasta do .ipynb # <-- troque aqui\n",
    "\n",
    "texto = ler_pdf(caminho_pdf)\n",
    "chunks = criar_chunks(texto)\n",
    "\n",
    "# Salva chunks para uso offline\n",
    "with open(\"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"{len(chunks)} chunks criados e salvos.\")\n",
    "\n",
    "# Gera embeddings\n",
    "embeddings = gerar_embeddings(chunks)\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Cria FAISS\n",
    "index = criar_faiss(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa257324",
   "metadata": {},
   "source": [
    "### üìå 8. Carregar tudo offline depois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fdf9b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks e √≠ndice carregados!\n"
     ]
    }
   ],
   "source": [
    "# Recarregar chunks e √≠ndice FAISS sem precisar refazer tudo\n",
    "def carregar_tudo():\n",
    "    chunks = json.load(open(\"chunks.json\", \"r\", encoding=\"utf-8\"))\n",
    "    index = faiss.read_index(\"faiss_index.bin\")\n",
    "    return chunks, index\n",
    "\n",
    "chunks, index = carregar_tudo()\n",
    "print(\"Chunks e √≠ndice carregados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb416b78",
   "metadata": {},
   "source": [
    "### üìå 9. Buscar contexto relevante no PDF via FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b235016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca vetorial\n",
    "def buscar(query, k=3):\n",
    "    query_emb = modelo_emb.encode([query]).astype(\"float32\")\n",
    "    dist, idx = index.search(query_emb, k)\n",
    "    resultados = [chunks[i] for i in idx[0]]\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb0386",
   "metadata": {},
   "source": [
    "### üìå 10. Carregar modelo LLM offline para responder (FLAN-T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0296a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429171f62c1f4d6ab100babbdc17cacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AULAML\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Erik\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: 6b86b1cb-90a6-4e26-9bb9-adfdb5f41f9b)')' thrown while requesting GET https://huggingface.co/google/flan-t5-small/resolve/main/model.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930127ee5b344ca79f82941e438e57b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1522bec494d4ddbaf442552e5f281ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec8e3ed84254b84b43b3651af0a66e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c6f14f919245be88a91f5cdf10d408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc1882e91924fc092c5fbd445c5ffab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06634284098c4864be5dff715cec62ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carrega modelo de linguagem 100% offline\n",
    "modelo = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "def gerar_resposta(prompt):\n",
    "    entrada = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    saida = modelo.generate(**entrada, max_length=300)\n",
    "    return tokenizer.decode(saida[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b6be0",
   "metadata": {},
   "source": [
    "### üìå 11. Fazer perguntas sobre o PDF (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c0befc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a wolf, a hunter, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf, a wolf\n"
     ]
    }
   ],
   "source": [
    "# Interface final do sistema RAG offline\n",
    "def perguntar(pergunta):\n",
    "    contexto = \"\\n\\n\".join(buscar(pergunta))\n",
    "    prompt = f\"\"\"\n",
    "Voc√™ √© um assistente. Use somente o contexto abaixo para responder:\n",
    "\n",
    "Contexto:\n",
    "{contexto}\n",
    "\n",
    "Pergunta: {pergunta}\n",
    "Resposta:\n",
    "\"\"\"\n",
    "    return gerar_resposta(prompt)\n",
    "\n",
    "# Teste\n",
    "# pergunta = \"Qual √© o resumo do PDF?\"\n",
    "pergunta = \"Who are the characters?\"\n",
    "print(perguntar(pergunta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bedf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
